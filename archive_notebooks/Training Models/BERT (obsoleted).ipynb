{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import warnings\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59848</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>cool like would want mother read realli great ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59849</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>thank would make life lot less anxietyinduc ke...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59852</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>urgent design problem kudo take impress</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>someth ill abl instal site releas</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59856</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha guy bunch loser</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                       comment_text  \\\n",
       "id                                                                   \n",
       "59848  0.000000  cool like would want mother read realli great ...   \n",
       "59849  0.000000  thank would make life lot less anxietyinduc ke...   \n",
       "59852  0.000000            urgent design problem kudo take impress   \n",
       "59855  0.000000                  someth ill abl instal site releas   \n",
       "59856  0.893617                               haha guy bunch loser   \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack   insult  threat  asian  \\\n",
       "id                                                                         \n",
       "59848         0.000000      0.0         0.000000  0.00000     0.0    0.0   \n",
       "59849         0.000000      0.0         0.000000  0.00000     0.0    0.0   \n",
       "59852         0.000000      0.0         0.000000  0.00000     0.0    0.0   \n",
       "59855         0.000000      0.0         0.000000  0.00000     0.0    0.0   \n",
       "59856         0.021277      0.0         0.021277  0.87234     0.0    0.0   \n",
       "\n",
       "       atheist  bisexual  ...  article_id    rating  funny  wow  sad  likes  \\\n",
       "id                        ...                                                 \n",
       "59848      0.0       0.0  ...        2006  rejected      0    0    0      0   \n",
       "59849      0.0       0.0  ...        2006  rejected      0    0    0      0   \n",
       "59852      0.0       0.0  ...        2006  rejected      0    0    0      0   \n",
       "59855      0.0       0.0  ...        2006  rejected      0    0    0      0   \n",
       "59856      0.0       0.0  ...        2006  rejected      0    0    0      1   \n",
       "\n",
       "       disagree  sexual_explicit  identity_annotator_count  \\\n",
       "id                                                           \n",
       "59848         0              0.0                         0   \n",
       "59849         0              0.0                         0   \n",
       "59852         0              0.0                         0   \n",
       "59855         0              0.0                         0   \n",
       "59856         0              0.0                         4   \n",
       "\n",
       "       toxicity_annotator_count  \n",
       "id                               \n",
       "59848                         4  \n",
       "59849                         4  \n",
       "59852                         4  \n",
       "59855                         4  \n",
       "59856                        47  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('cleaned_train.csv', index_col='id')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1860808710928454067</th>\n",
       "      <td>thefrogsoupram bluerei naegiko didnt consent a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860798467011977633</th>\n",
       "      <td>kayninewrit rowanisr softagatha someon could w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860783005717176665</th>\n",
       "      <td>ai art revolut threat discuss ethic implic aig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860698588394971197</th>\n",
       "      <td>ai music video wake wildflow music video wake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860694275467403362</th>\n",
       "      <td>skullxnft great concept honestli origin ethic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text\n",
       "id                                                                    \n",
       "1860808710928454067  thefrogsoupram bluerei naegiko didnt consent a...\n",
       "1860798467011977633  kayninewrit rowanisr softagatha someon could w...\n",
       "1860783005717176665  ai art revolut threat discuss ethic implic aig...\n",
       "1860698588394971197  ai music video wake wildflow music video wake ...\n",
       "1860694275467403362  skullxnft great concept honestli origin ethic ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('all_data.csv', index_col='id')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>...</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.084552</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.014445</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>0.065036</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>...</td>\n",
       "      <td>133455.208857</td>\n",
       "      <td>58306.749238</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.618952</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>1.687714</td>\n",
       "      <td>8.535524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.179662</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.072130</td>\n",
       "      <td>0.069335</td>\n",
       "      <td>0.156771</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.032361</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>0.018030</td>\n",
       "      <td>0.079135</td>\n",
       "      <td>...</td>\n",
       "      <td>127320.131102</td>\n",
       "      <td>22846.173068</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.925190</td>\n",
       "      <td>0.006901</td>\n",
       "      <td>0.044362</td>\n",
       "      <td>31.246439</td>\n",
       "      <td>50.847671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42746.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>241888.000000</td>\n",
       "      <td>52124.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255676.750000</td>\n",
       "      <td>55785.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>267814.000000</td>\n",
       "      <td>106343.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.948495</td>\n",
       "      <td>1842.000000</td>\n",
       "      <td>1867.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             target  severe_toxicity       obscene  identity_attack  \\\n",
       "count  21000.000000     21000.000000  21000.000000     21000.000000   \n",
       "mean       0.084552         0.003952      0.014445         0.017568   \n",
       "std        0.179662         0.021774      0.072130         0.069335   \n",
       "min        0.000000         0.000000      0.000000         0.000000   \n",
       "25%        0.000000         0.000000      0.000000         0.000000   \n",
       "50%        0.000000         0.000000      0.000000         0.000000   \n",
       "75%        0.100000         0.000000      0.000000         0.000000   \n",
       "max        1.000000         0.300000      1.000000         1.000000   \n",
       "\n",
       "             insult        threat         asian       atheist      bisexual  \\\n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean       0.065036      0.008954      0.001489      0.001174      0.000713   \n",
       "std        0.156771      0.048099      0.032361      0.031542      0.018030   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              black  ...      parent_id     article_id         funny  \\\n",
       "count  21000.000000  ...   21000.000000   21000.000000  21000.000000   \n",
       "mean       0.007602  ...  133455.208857   58306.749238      0.000476   \n",
       "std        0.079135  ...  127320.131102   22846.173068      0.021817   \n",
       "min        0.000000  ...       0.000000    2006.000000      0.000000   \n",
       "25%        0.000000  ...       0.000000   42746.250000      0.000000   \n",
       "50%        0.000000  ...  241888.000000   52124.000000      0.000000   \n",
       "75%        0.000000  ...  255676.750000   55785.000000      0.000000   \n",
       "max        1.000000  ...  267814.000000  106343.000000      1.000000   \n",
       "\n",
       "                wow      sad         likes      disagree  sexual_explicit  \\\n",
       "count  21000.000000  21000.0  21000.000000  21000.000000     21000.000000   \n",
       "mean       0.000048      0.0      1.618952      0.000048         0.006298   \n",
       "std        0.006901      0.0      2.925190      0.006901         0.044362   \n",
       "min        0.000000      0.0      0.000000      0.000000         0.000000   \n",
       "25%        0.000000      0.0      0.000000      0.000000         0.000000   \n",
       "50%        0.000000      0.0      1.000000      0.000000         0.000000   \n",
       "75%        0.000000      0.0      2.000000      0.000000         0.000000   \n",
       "max        1.000000      0.0     60.000000      1.000000         0.948495   \n",
       "\n",
       "       identity_annotator_count  toxicity_annotator_count  \n",
       "count              21000.000000              21000.000000  \n",
       "mean                   1.687714                  8.535524  \n",
       "std                   31.246439                 50.847671  \n",
       "min                    0.000000                  4.000000  \n",
       "25%                    0.000000                  4.000000  \n",
       "50%                    0.000000                  4.000000  \n",
       "75%                    0.000000                  6.000000  \n",
       "max                 1842.000000               1867.000000  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21000 entries, 59848 to 267822\n",
      "Data columns (total 44 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   target                               21000 non-null  float64\n",
      " 1   comment_text                         20950 non-null  object \n",
      " 2   severe_toxicity                      21000 non-null  float64\n",
      " 3   obscene                              21000 non-null  float64\n",
      " 4   identity_attack                      21000 non-null  float64\n",
      " 5   insult                               21000 non-null  float64\n",
      " 6   threat                               21000 non-null  float64\n",
      " 7   asian                                21000 non-null  float64\n",
      " 8   atheist                              21000 non-null  float64\n",
      " 9   bisexual                             21000 non-null  float64\n",
      " 10  black                                21000 non-null  float64\n",
      " 11  buddhist                             21000 non-null  float64\n",
      " 12  christian                            21000 non-null  float64\n",
      " 13  female                               21000 non-null  float64\n",
      " 14  heterosexual                         21000 non-null  float64\n",
      " 15  hindu                                21000 non-null  float64\n",
      " 16  homosexual_gay_or_lesbian            21000 non-null  float64\n",
      " 17  intellectual_or_learning_disability  21000 non-null  float64\n",
      " 18  jewish                               21000 non-null  float64\n",
      " 19  latino                               21000 non-null  float64\n",
      " 20  male                                 21000 non-null  float64\n",
      " 21  muslim                               21000 non-null  float64\n",
      " 22  other_disability                     21000 non-null  float64\n",
      " 23  other_gender                         21000 non-null  float64\n",
      " 24  other_race_or_ethnicity              21000 non-null  float64\n",
      " 25  other_religion                       21000 non-null  float64\n",
      " 26  other_sexual_orientation             21000 non-null  float64\n",
      " 27  physical_disability                  21000 non-null  float64\n",
      " 28  psychiatric_or_mental_illness        21000 non-null  float64\n",
      " 29  transgender                          21000 non-null  float64\n",
      " 30  white                                21000 non-null  float64\n",
      " 31  created_date                         21000 non-null  object \n",
      " 32  publication_id                       21000 non-null  int64  \n",
      " 33  parent_id                            21000 non-null  float64\n",
      " 34  article_id                           21000 non-null  int64  \n",
      " 35  rating                               21000 non-null  object \n",
      " 36  funny                                21000 non-null  int64  \n",
      " 37  wow                                  21000 non-null  int64  \n",
      " 38  sad                                  21000 non-null  int64  \n",
      " 39  likes                                21000 non-null  int64  \n",
      " 40  disagree                             21000 non-null  int64  \n",
      " 41  sexual_explicit                      21000 non-null  float64\n",
      " 42  identity_annotator_count             21000 non-null  int64  \n",
      " 43  toxicity_annotator_count             21000 non-null  int64  \n",
      "dtypes: float64(32), int64(9), object(3)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 343 entries, 1860808710928454067 to 1859715514517553397\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    342 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                                  0\n",
       "comment_text                           50\n",
       "severe_toxicity                         0\n",
       "obscene                                 0\n",
       "identity_attack                         0\n",
       "insult                                  0\n",
       "threat                                  0\n",
       "asian                                   0\n",
       "atheist                                 0\n",
       "bisexual                                0\n",
       "black                                   0\n",
       "buddhist                                0\n",
       "christian                               0\n",
       "female                                  0\n",
       "heterosexual                            0\n",
       "hindu                                   0\n",
       "homosexual_gay_or_lesbian               0\n",
       "intellectual_or_learning_disability     0\n",
       "jewish                                  0\n",
       "latino                                  0\n",
       "male                                    0\n",
       "muslim                                  0\n",
       "other_disability                        0\n",
       "other_gender                            0\n",
       "other_race_or_ethnicity                 0\n",
       "other_religion                          0\n",
       "other_sexual_orientation                0\n",
       "physical_disability                     0\n",
       "psychiatric_or_mental_illness           0\n",
       "transgender                             0\n",
       "white                                   0\n",
       "created_date                            0\n",
       "publication_id                          0\n",
       "parent_id                               0\n",
       "article_id                              0\n",
       "rating                                  0\n",
       "funny                                   0\n",
       "wow                                     0\n",
       "sad                                     0\n",
       "likes                                   0\n",
       "disagree                                0\n",
       "sexual_explicit                         0\n",
       "identity_annotator_count                0\n",
       "toxicity_annotator_count                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                                 0\n",
       "comment_text                           0\n",
       "severe_toxicity                        0\n",
       "obscene                                0\n",
       "identity_attack                        0\n",
       "insult                                 0\n",
       "threat                                 0\n",
       "asian                                  0\n",
       "atheist                                0\n",
       "bisexual                               0\n",
       "black                                  0\n",
       "buddhist                               0\n",
       "christian                              0\n",
       "female                                 0\n",
       "heterosexual                           0\n",
       "hindu                                  0\n",
       "homosexual_gay_or_lesbian              0\n",
       "intellectual_or_learning_disability    0\n",
       "jewish                                 0\n",
       "latino                                 0\n",
       "male                                   0\n",
       "muslim                                 0\n",
       "other_disability                       0\n",
       "other_gender                           0\n",
       "other_race_or_ethnicity                0\n",
       "other_religion                         0\n",
       "other_sexual_orientation               0\n",
       "physical_disability                    0\n",
       "psychiatric_or_mental_illness          0\n",
       "transgender                            0\n",
       "white                                  0\n",
       "created_date                           0\n",
       "publication_id                         0\n",
       "parent_id                              0\n",
       "article_id                             0\n",
       "rating                                 0\n",
       "funny                                  0\n",
       "wow                                    0\n",
       "sad                                    0\n",
       "likes                                  0\n",
       "disagree                               0\n",
       "sexual_explicit                        0\n",
       "identity_annotator_count               0\n",
       "toxicity_annotator_count               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.dropna(inplace=True)\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test shape: (20950, 44) (342, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train and test shape: {} {}\".format(train_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = train_df[['comment_text']]\n",
    "output = train_df[['target']] \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, output, test_size=0.25, random_state=5400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_LEN = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT embedding generation and dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class ToxicityDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        target = self.targets[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"targets\": torch.tensor(target, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# data loader\n",
    "def create_data_loader(df, targets, tokenizer, max_len, batch_size):\n",
    "    dataset = ToxicityDataset(\n",
    "        texts=df['comment_text'].values,\n",
    "        targets=targets.values.flatten(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Regression Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, output_size):\n",
    "        super(BertRegressionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "\n",
    "model = BertRegressionModel(\"bert-base-uncased\", output_size=1).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 131/131 [05:05<00:00,  2.33s/it, loss=0.0715] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 131/131 [05:00<00:00,  2.29s/it, loss=0.0641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 131/131 [05:03<00:00,  2.31s/it, loss=0.0394] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# train\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target: obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 66/66 [04:38<00:00,  4.23s/it, loss=0.00152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 66/66 [02:49<00:00,  2.57s/it, loss=0.00935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 66/66 [02:30<00:00,  2.28s/it, loss=0.00351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Processing target: obscene\")\n",
    "\n",
    "# Update feature and output\n",
    "feature = train_df[[\"comment_text\"]]\n",
    "output = train_df[[\"obscene\"]]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, output, test_size=0.25, random_state=5400)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)  # Progress bar\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test set\n",
    "test_loader = create_data_loader(test_df, pd.DataFrame([0] * len(test_df), columns=[\"dummy_target\"]), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "predictions = []\n",
    "print(\"Starting predictions...\")\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)  # Progress bar for prediction\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save predictions\n",
    "test_df[\"predicted_obscene\"] = predictions\n",
    "print(\"Predictions for 'obscene' added to test_df.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data/data-predict/BERT_test_predictions_obscene.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identity_attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target: identity_attack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 2/982 [00:06<56:44,  3.47s/it, loss=0.00703]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m---> 27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Processing target: identity_attack\")\n",
    "\n",
    "# Update feature and output\n",
    "feature = train_df[[\"comment_text\"]]\n",
    "output = train_df[[\"identity_attack\"]]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, output, test_size=0.25, random_state=5400)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)  # Progress bar\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test set\n",
    "test_loader = create_data_loader(test_df, pd.DataFrame([0] * len(test_df), columns=[\"dummy_target\"]), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "predictions = []\n",
    "print(\"Starting predictions...\")\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)  # Progress bar for prediction\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "test_df[\"predicted_obscene\"] = predictions\n",
    "print(\"Predictions for 'obscene' added to test_df.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data/data-predict/BERT_test_predictions_identity_attack.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Processing target: insult\")\n",
    "\n",
    "# Update feature and output\n",
    "feature = train_df[[\"comment_text\"]]\n",
    "output = train_df[[\"insult\"]]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, output, test_size=0.25, random_state=5400)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)  # Progress bar\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test set\n",
    "test_loader = create_data_loader(test_df, pd.DataFrame([0] * len(test_df), columns=[\"dummy_target\"]), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "predictions = []\n",
    "print(\"Starting predictions...\")\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)  # Progress bar for prediction\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "test_df[\"predicted_obscene\"] = predictions\n",
    "print(\"Predictions for 'obscene' added to test_df.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data/data-predict/BERT_test_predictions_insult.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Processing target: threat\")\n",
    "\n",
    "# Update feature and output\n",
    "feature = train_df[[\"comment_text\"]]\n",
    "output = train_df[[\"threat\"]]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature, output, test_size=0.25, random_state=5400)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loader(X_train, y_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(X_val, y_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)  # Progress bar\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Average Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        targets = data[\"targets\"].to(DEVICE).unsqueeze(1)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test set\n",
    "test_loader = create_data_loader(test_df, pd.DataFrame([0] * len(test_df), columns=[\"dummy_target\"]), tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "predictions = []\n",
    "print(\"Starting predictions...\")\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader, leave=True)  # Progress bar for prediction\n",
    "    for data in loop:\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions.extend(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "test_df[\"predicted_obscene\"] = predictions\n",
    "print(\"Predictions for 'obscene' added to test_df.\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"data/data-predict/BERT_test_predictions_insult.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
