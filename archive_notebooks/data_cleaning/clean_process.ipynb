{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducion\n",
    "This script processes raw tweet data to prepare it for analysis by cleaning and standardizing the text. It removes unwanted elements such as hashtags, mentions, links, and non-English words, while also converting text to lowercase and eliminating duplicates. Using NLTK, it filters out stopwords and applies stemming to reduce words to their root forms for more effective analysis. The cleaned tweets are then organized into a Pandas DataFrame, with unnecessary columns removed, and exported as a CSV file (all_data.csv).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xunlei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# download NLTK çš„ stopwords data\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # 1. Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # 2. Remove newline characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace characters (including newline, tabs) with a single space\n",
    "    \n",
    "    # 3. Remove meaningless characters, such as special symbols (can be adjusted as needed)\n",
    "    text = re.sub(r'[^\\w\\s,.\\-]', '', text)  # Keep letters, numbers, spaces, commas, periods, and hyphens\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'@\\S+', '', text)  # Remove @username mentions\n",
    "    \n",
    "    # 4. Optional: Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "\n",
    "    # 5. If you want to remove tags (e.g., # or @), you can add this step\n",
    "    text = re.sub(r'#[\\w]+', '', text)  # Remove all hashtags (e.g., #aiart)\n",
    "    text = re.sub(r'@\\S+', '', text)    # Remove all usernames (e.g., @ai_ethics)\n",
    "    text = re.sub(r'-', '', text) \n",
    "    return text\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "    # English words\n",
    "    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Remove stop words and apply stemming\n",
    "    processed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    # Join the processed words back into a string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Open and read the original data file\n",
    "data = []\n",
    "with open('../data/data-raw/raw_tweets.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Clean the text of each tweet\n",
    "for tweet in data:\n",
    "    tweet['text'] = clean_text(tweet['text'])          # Clean the 'text' field\n",
    "    tweet['text'] = remove_non_english_words(tweet['text'])  # Keep only English words\n",
    "    tweet['text'] = process_text(tweet['text'])        # Remove stop words and apply stemming\n",
    "\n",
    "# Use pandas to convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop the 'created_at' column\n",
    "df = df.drop(columns=['created_at'])\n",
    "\n",
    "# Drop duplicate rows based on the 'text' column\n",
    "df = df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Output the DataFrame to a CSV file\n",
    "df.to_csv('all_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Data cleaned and written to all_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and written to cleaned_train.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df = df[df['comment_text'].notna() & (df['comment_text'] != '')]\n",
    "\n",
    "df = df.fillna(0)\n",
    "df['comment_text'] = df['comment_text'].astype(str)\n",
    "\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(clean_text)           \n",
    "df['comment_text'] = df['comment_text'].apply(remove_non_english_words)  \n",
    "df['comment_text'] = df['comment_text'].apply(process_text)\n",
    "\n",
    "\n",
    "df.to_csv('cleaned_train.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Data cleaned and written to cleaned_train.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
